{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunkr24/Natural-Language-Processing/blob/Python/Sentiment%20Analyses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B0GTQ1hrgkU"
      },
      "source": [
        "**CONTEXT:**  \n",
        "The objective of this project is to build a text classification model that\n",
        "analyses the customer's sentiments based on their reviews in the IMDB database. The\n",
        "model uses a complex deep learning model to build an embedding layer followed by\n",
        "a classification algorithm to analyse the sentiment of the customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IEjyk2vrn_x"
      },
      "source": [
        "**OBJECTIVE:**  \n",
        "Build a sequential NLP classifier which can use input text\n",
        "parameters to determine the customer sentiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOy7I_DfruPv"
      },
      "source": [
        "**DATA DESCRIPTION:**    \n",
        "The Dataset of 50,000 movie reviews from IMDB, labelled by\n",
        "sentiment (positive/negative). Reviews have been preprocessed, and each review is\n",
        "encoded as a sequence of word indexes (integers). For convenience, the words are\n",
        "indexed by their frequency in the dataset, meaning the for that has index 1 is the\n",
        "most frequent word. Use the first 20 words from each review to speed up training,\n",
        "using a max vocabulary size of 10,000. As a convention, \"0\" does not stand for a\n",
        "specific word, but instead is used to encode any unknown word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7I51KIHr9xY"
      },
      "source": [
        "**Load IMDB Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGQKZWW5GQdS"
      },
      "source": [
        "from keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAQ171mAd9-y"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 10000 #vocab size\n",
        "maxlen = 30  #number of word used from each review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST1PYIGCeJEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cae7a90-ad3f-4731-d8b8-3e967425976d"
      },
      "source": [
        "#load dataset as a list of ints\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "#make all sequences of the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBX3WlMMePOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d136c9-42ae-4346-ce2f-7574700b45af"
      },
      "source": [
        "print (x_train[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 371   78   22  625   64 1382    9    8  168  145   23    4 1690   15\n",
            "   16    4 1355    5   28    6   52  154  462   33   89   78  285   16\n",
            "  145   95]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxfafYF_eU5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039d9cca-a4a2-4777-db71-b21405e9e5e3"
      },
      "source": [
        "print (x_train[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvJlzvfqeYmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd314330-2a95-40ff-dbc5-0361cb49fb0d"
      },
      "source": [
        "print(y_train[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMYqGPsdecc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65664448-2879-4d49-c3a6-14f53d0734e9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0     1]\n",
            " [12500 12500]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4WhU1SXenlg"
      },
      "source": [
        "**WORD INDEX BUILDING**    \n",
        "Get the word index and then Create a key-value pair for word and word_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtf47ZdieclI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c60d20-e670-4b43-acca-50497e4b169a"
      },
      "source": [
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc9VFwHZecoB"
      },
      "source": [
        "#ref :: https://stackoverflow.com/questions/41971587/how-to-convert-predicted-sequence-back-to-text-in-keras\n",
        "reverse_word_map = dict(map(reversed, word_index.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBQkmmLzecrG"
      },
      "source": [
        "# Function takes a tokenized sentence and returns the words\n",
        "def sequence_to_text(list_of_indices):\n",
        "    # Looking up words in dictionary\n",
        "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "    return(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVM4jFmUftgR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29dd8492-dc78-4e37-c7f6-e3bb37e8873f"
      },
      "source": [
        "#test\n",
        "review = sequence_to_text(x_train[0])\n",
        "print(review)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'heartfelt', 'film', 'want', 'an']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-tbj50Cf-O4"
      },
      "source": [
        "**Build Keras Embedding Layer Model**     \n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:    \n",
        "\n",
        "The embedding layer can be used at the start of a larger deep learning model.    \n",
        "Also we could load pre-train word embeddings into the embedding layer when we create our model.    \n",
        "Use the embedding layer to train our own word2vec models.    \n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn LabelEncoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abnTZNXmecuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830bfb57-ea23-4d6c-df4a-81edaf883949"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import LSTM\n",
        "### create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 128, trainable=True, input_length=maxlen))\n",
        "model.add(LSTM(units=64, dropout=0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "### Fit the model\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=500, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 128)           1280000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,331,521\n",
            "Trainable params: 1,331,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 15s 263ms/step - loss: 0.5972 - accuracy: 0.6791 - val_loss: 0.4614 - val_accuracy: 0.7792\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3941 - accuracy: 0.8228 - val_loss: 0.4470 - val_accuracy: 0.7877\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3253 - accuracy: 0.8596 - val_loss: 0.4645 - val_accuracy: 0.7829\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2716 - accuracy: 0.8895 - val_loss: 0.5077 - val_accuracy: 0.7758\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2185 - accuracy: 0.9126 - val_loss: 0.6201 - val_accuracy: 0.7691\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1727 - accuracy: 0.9314 - val_loss: 0.7166 - val_accuracy: 0.7605\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 12s 251ms/step - loss: 0.1424 - accuracy: 0.9426 - val_loss: 0.8207 - val_accuracy: 0.7599\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 12s 250ms/step - loss: 0.1211 - accuracy: 0.9496 - val_loss: 0.8927 - val_accuracy: 0.7560\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 12s 250ms/step - loss: 0.0996 - accuracy: 0.9584 - val_loss: 1.1643 - val_accuracy: 0.7520\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.0878 - accuracy: 0.9644 - val_loss: 1.1754 - val_accuracy: 0.7545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2da994fc10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vil--Dg7hXqm"
      },
      "source": [
        "**Model Accuracy**   \n",
        "Report the Accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6IVwM-FecxX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260ed422-d653-445d-f870-b42817d36a85"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 75.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEgZ9bwYec0c"
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu2dSXM2ec3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0dec8b-3011-4b10-a630-95ca7f97f1d7"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.9546659e-01]\n",
            " [9.9999595e-01]\n",
            " [9.9616325e-01]\n",
            " ...\n",
            " [5.8326125e-04]\n",
            " [6.6266179e-02]\n",
            " [9.9995315e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUobA9Lcec5t"
      },
      "source": [
        "y_pred = np.round(y_pred, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zVIB2oZec9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440dede3-50bf-4b4d-c766-d03994a2b330"
      },
      "source": [
        "y_pred = y_pred.ravel()\n",
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HvC1P8aiqaG"
      },
      "source": [
        "y_pred = y_pred.astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDjJL3E7iyuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2fed2c0-1a9b-406b-f940-7e25c3cf8683"
      },
      "source": [
        "y_test.ravel()\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K8D4eLtswRr"
      },
      "source": [
        "**Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPcSRAPiiy70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603429a7-042f-4d12-d5f7-96d7e531b2ff"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['Sentiment_Positive', 'Sentiment_Negative']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Sentiment_Positive       0.75      0.76      0.76     12500\n",
            "Sentiment_Negative       0.76      0.75      0.75     12500\n",
            "\n",
            "          accuracy                           0.75     25000\n",
            "         macro avg       0.75      0.75      0.75     25000\n",
            "      weighted avg       0.75      0.75      0.75     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1yMwQhBizFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6015749c-12c6-43ae-e5ed-2011b591221f"
      },
      "source": [
        "sequence_to_text(x_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['by',\n",
              " 'are',\n",
              " 'be',\n",
              " 'favourites',\n",
              " 'all',\n",
              " 'family',\n",
              " 'turn',\n",
              " 'in',\n",
              " 'does',\n",
              " 'as',\n",
              " 'three',\n",
              " 'part',\n",
              " 'in',\n",
              " 'another',\n",
              " 'some',\n",
              " 'to',\n",
              " 'be',\n",
              " 'probably',\n",
              " 'with',\n",
              " 'world',\n",
              " 'and',\n",
              " 'her',\n",
              " 'an',\n",
              " 'have',\n",
              " 'faint',\n",
              " 'beginning',\n",
              " 'own',\n",
              " 'as',\n",
              " 'is',\n",
              " 'sequence']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMn2RwBWizMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb225268-4a8a-495c-fc26-c6982663460d"
      },
      "source": [
        "sequence_to_text(x_test[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good',\n",
              " '2',\n",
              " 'which',\n",
              " 'why',\n",
              " 'super',\n",
              " 'as',\n",
              " 'it',\n",
              " 'main',\n",
              " 'of',\n",
              " 'my',\n",
              " 'i',\n",
              " 'i',\n",
              " '\\x96',\n",
              " 'if',\n",
              " 'time',\n",
              " 'screenplay',\n",
              " 'in',\n",
              " 'same',\n",
              " 'this',\n",
              " 'remember',\n",
              " 'assured',\n",
              " 'have',\n",
              " 'action',\n",
              " 'one',\n",
              " 'in',\n",
              " 'realistic',\n",
              " 'that',\n",
              " 'better',\n",
              " 'of',\n",
              " 'lessons']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsrsz34-izS0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}